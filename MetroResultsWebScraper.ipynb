{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4129a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 2016-17 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2016-17&sportid=9\n",
      "Scraping 2017-18 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2017-18&sportid=9\n",
      "Scraping 2018-19 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2018-19&sportid=9\n",
      "Scraping 2019-20 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2019-20&sportid=9\n",
      "Scraping 2020-21 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2020-21&sportid=9\n",
      "Scraping 2021-22 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2021-22&sportid=9\n",
      "Scraping 2022-23 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2022-23&sportid=9\n",
      "Scraping 2023-24 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2023-24&sportid=9\n",
      "Scraping 2024-25 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2024-25&sportid=9\n",
      "Scraping 2025-26 → https://www.metroleaguewa.org/sport/?leagueid=8&level_id=12&school_year=2025-26&sportid=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6236\\412716844.py:138: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dt = pd.to_datetime(f\"{d} {t}\", errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6236\\412716844.py:142: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  d_only = pd.to_datetime(d, errors=\"coerce\", infer_datetime_format=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: C:\\Users\\User\\OneDrive - 2020 Companies\\Desktop\\IHS Soccer\\Boys\\2026\\metroleague_results_all_games.csv\n",
      "\n",
      "Unique DateRaw sample:\n",
      "  Thursday, Mar 9\n",
      "  Friday, Mar 10\n",
      "  Saturday, Mar 11\n",
      "  Tuesday, Mar 14\n",
      "  Wednesday, Mar 15\n",
      "  Friday, Mar 17\n",
      "  Monday, Mar 20\n",
      "  Wednesday, Mar 22\n",
      "  Friday, Mar 24\n",
      "  Tuesday, Mar 28\n",
      "  Thursday, Mar 30\n",
      "  Friday, Mar 31\n",
      "  Monday, Apr 3\n",
      "  Wednesday, Apr 5\n",
      "  Friday, Apr 7\n",
      "  Monday, Apr 10\n",
      "  Wednesday, Apr 12\n",
      "  Monday, Apr 17\n",
      "  Tuesday, Apr 18\n",
      "  Wednesday, Apr 19\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import hashlib\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "LEAGUE_ID = 8\n",
    "SPORT_ID = 9\n",
    "\n",
    "MIN_SEASON = \"2016-17\"\n",
    "HEADLESS = False\n",
    "WAIT = 30\n",
    "\n",
    "# Varsity is often 12; keep a couple fallbacks\n",
    "LEVEL_ID_CANDIDATES = [12, 11, 10, 13]\n",
    "\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\User\\OneDrive - 2020 Companies\\Desktop\\IHS Soccer\\Boys\\2026\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEBUG_DIR = OUTPUT_DIR / \"debug\"\n",
    "DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEAM_NAME_CORRECTIONS = {\n",
    "    \"Lakeside\": \"Lakeside (Seattle)\",\n",
    "    \"Lakeside (Sea)\": \"Lakeside (Seattle)\",\n",
    "    \"Seattle Prep.\": \"Seattle Prep\",\n",
    "}\n",
    "\n",
    "ORDINAL_SUFFIX_RE = re.compile(r\"(\\d+)(st|nd|rd|th)\\b\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Driver\n",
    "# =========================\n",
    "def build_driver(headless: bool) -> webdriver.Chrome:\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1400,900\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # Helps on JS-heavy pages sometimes\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Season generation (no dropdown needed)\n",
    "# =========================\n",
    "def current_school_year(today: datetime.date | None = None) -> str:\n",
    "    today = today or datetime.date.today()\n",
    "    # rollover ~ July (good enough for school-year inference)\n",
    "    start = today.year if today.month >= 7 else today.year - 1\n",
    "    return f\"{start}-{str((start + 1) % 100).zfill(2)}\"\n",
    "\n",
    "\n",
    "def generate_school_years(min_season: str) -> list[str]:\n",
    "    start_year = int(min_season.split(\"-\")[0])\n",
    "    end_year = int(current_school_year().split(\"-\")[0])\n",
    "    return [f\"{y}-{str((y + 1) % 100).zfill(2)}\" for y in range(start_year, end_year + 1)]\n",
    "\n",
    "\n",
    "def season_urls() -> list[tuple[str, int, str]]:\n",
    "    urls = []\n",
    "    for y in generate_school_years(MIN_SEASON):\n",
    "        for level_id in LEVEL_ID_CANDIDATES:\n",
    "            url = (\n",
    "                f\"https://www.metroleaguewa.org/sport/?leagueid={LEAGUE_ID}\"\n",
    "                f\"&level_id={level_id}&school_year={y}&sportid={SPORT_ID}\"\n",
    "            )\n",
    "            urls.append((y, level_id, url))\n",
    "    # ensure earliest season first, and level_id order per season\n",
    "    urls.sort(key=lambda x: (int(x[0].split(\"-\")[0]), LEVEL_ID_CANDIDATES.index(x[1])))\n",
    "    return urls\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Normalization / IDs\n",
    "# =========================\n",
    "def normalize_team_name(name: str) -> str:\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    name = name.strip()\n",
    "    return TEAM_NAME_CORRECTIONS.get(name, name)\n",
    "\n",
    "\n",
    "def generate_game_id(season: str, date_raw: str, time_raw: str, home: str, away: str) -> str:\n",
    "    s = f\"{season}_{date_raw}_{time_raw}_{home}_{away}\"\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Date parsing\n",
    "# =========================\n",
    "def season_years(season: str) -> tuple[int, int]:\n",
    "    a, b = season.split(\"-\")\n",
    "    start = int(a)\n",
    "    end = int(b)\n",
    "    if end < 100:\n",
    "        end += 2000\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def infer_year_from_season_and_month(season: str, month: int, rollover_month: int = 7) -> int:\n",
    "    start, end = season_years(season)\n",
    "    return start if month >= rollover_month else end\n",
    "\n",
    "\n",
    "def parse_match_datetime(season: str, date_heading: str, time_str: str) -> pd.Timestamp:\n",
    "    if not isinstance(date_heading, str) or not date_heading.strip():\n",
    "        return pd.NaT\n",
    "\n",
    "    d = date_heading.strip()\n",
    "    d = ORDINAL_SUFFIX_RE.sub(r\"\\1\", d)\n",
    "    d = re.sub(r\"^\\w+,\\s*\", \"\", d)  # remove weekday prefix like \"Saturday,\"\n",
    "    d = re.sub(r\"\\s+\", \" \", d)\n",
    "\n",
    "    t = (time_str or \"\").strip()\n",
    "    if not t or t.upper() == \"TBD\":\n",
    "        t = \"12:00 PM\"\n",
    "\n",
    "    # Try if year is present (sometimes it is)\n",
    "    dt = pd.to_datetime(f\"{d} {t}\", errors=\"coerce\", infer_datetime_format=True)\n",
    "    if pd.notna(dt) and dt.year > 1900:\n",
    "        return dt\n",
    "\n",
    "    d_only = pd.to_datetime(d, errors=\"coerce\", infer_datetime_format=True)\n",
    "    if pd.isna(d_only):\n",
    "        return pd.NaT\n",
    "\n",
    "    inferred_year = infer_year_from_season_and_month(season, int(d_only.month))\n",
    "    return pd.to_datetime(\n",
    "        f\"{inferred_year}-{int(d_only.month):02d}-{int(d_only.day):02d} {t}\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Context helpers (iframe-safe)\n",
    "# =========================\n",
    "def _has_any(driver, css: str) -> bool:\n",
    "    return len(driver.find_elements(By.CSS_SELECTOR, css)) > 0\n",
    "\n",
    "\n",
    "def switch_to_context_with(driver, css_list: list[str], timeout: int = WAIT) -> bool:\n",
    "    \"\"\"\n",
    "    Tries default content + iframes until one contains any selector in css_list.\n",
    "    Leaves driver focused in the matching context if found.\n",
    "    \"\"\"\n",
    "    end = time.time() + timeout\n",
    "\n",
    "    def found_here() -> bool:\n",
    "        return any(_has_any(driver, css) for css in css_list)\n",
    "\n",
    "    while time.time() < end:\n",
    "        driver.switch_to.default_content()\n",
    "        if found_here():\n",
    "            return True\n",
    "\n",
    "        frames = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        for f in frames:\n",
    "            try:\n",
    "                driver.switch_to.default_content()\n",
    "                driver.switch_to.frame(f)\n",
    "                if found_here():\n",
    "                    return True\n",
    "\n",
    "                # one nested level (just in case)\n",
    "                nested = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "                for nf in nested:\n",
    "                    try:\n",
    "                        driver.switch_to.frame(nf)\n",
    "                        if found_here():\n",
    "                            return True\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    finally:\n",
    "                        try:\n",
    "                            driver.switch_to.parent_frame()\n",
    "                        except Exception:\n",
    "                            driver.switch_to.default_content()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        time.sleep(0.4)\n",
    "\n",
    "    driver.switch_to.default_content()\n",
    "    return False\n",
    "\n",
    "\n",
    "def save_debug(driver, season: str, level_id: int, tag: str):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base = DEBUG_DIR / f\"{season}_level{level_id}_{tag}_{ts}\"\n",
    "    try:\n",
    "        (base.with_suffix(\".html\")).write_text(driver.page_source, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        driver.save_screenshot(str(base.with_suffix(\".png\")))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Full season filter\n",
    "# =========================\n",
    "def set_full_season_filter(driver) -> bool:\n",
    "    \"\"\"\n",
    "    Uses the confirmed control:\n",
    "      <select id=\"filter_date_range_kword\"> ... value=\"season\" ...\n",
    "    Works in whichever context (default or iframe) contains it.\n",
    "    \"\"\"\n",
    "    ok = switch_to_context_with(driver, [\"#filter_date_range_kword\"], timeout=10)\n",
    "    if not ok:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        sel_el = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"filter_date_range_kword\")))\n",
    "        sel = Select(sel_el)\n",
    "        cur = (sel.first_selected_option.get_attribute(\"value\") or \"\").strip().lower()\n",
    "        if cur != \"season\":\n",
    "            # staleness wait on some existing schedule node if present\n",
    "            old = None\n",
    "            for css in [\".schedule_contents\", \".event_row\", \".schedule_date_heading\"]:\n",
    "                els = driver.find_elements(By.CSS_SELECTOR, css)\n",
    "                if els:\n",
    "                    old = els[0]\n",
    "                    break\n",
    "            sel.select_by_value(\"season\")\n",
    "            if old is not None:\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(EC.staleness_of(old))\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_lazy(driver, rounds: int = 12):\n",
    "    \"\"\"\n",
    "    If full season still lazy-loads, scrolling often forces additional dates to render.\n",
    "    \"\"\"\n",
    "    last_rows = -1\n",
    "    for _ in range(rounds):\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, \".event_row\")\n",
    "        if len(rows) == last_rows:\n",
    "            break\n",
    "        last_rows = len(rows)\n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(0.7)\n",
    "    try:\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(0.3)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Scrape season with correct per-row dates\n",
    "# =========================\n",
    "def scrape_season(driver, season: str, level_id: int, url: str) -> list[dict]:\n",
    "    driver.switch_to.default_content()\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, WAIT).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(1.0)\n",
    "\n",
    "    # Ensure we're in the context that contains the schedule content\n",
    "    ok = switch_to_context_with(driver, [\".schedule_date_heading\", \".event_row\", \".schedule_contents\", \"#filter_date_range_kword\"], timeout=WAIT)\n",
    "    if not ok:\n",
    "        save_debug(driver, season, level_id, \"no_schedule_context\")\n",
    "        return []\n",
    "\n",
    "    # Force Full Season if possible\n",
    "    set_full_season_filter(driver)\n",
    "\n",
    "    # Re-locate schedule context (it may rerender)\n",
    "    switch_to_context_with(driver, [\".schedule_date_heading\", \".event_row\", \".schedule_contents\"], timeout=15)\n",
    "\n",
    "    # Let rows populate\n",
    "    load_lazy(driver)\n",
    "\n",
    "    rows = driver.find_elements(By.CSS_SELECTOR, \".event_row\")\n",
    "    if not rows:\n",
    "        save_debug(driver, season, level_id, \"no_rows\")\n",
    "        return []\n",
    "\n",
    "    matches = []\n",
    "    row_index = 0\n",
    "    current_date = None\n",
    "\n",
    "    # Best method: walk headings + rows in DOM order\n",
    "    items = driver.find_elements(By.XPATH, \"//*[contains(@class,'schedule_date_heading') or contains(@class,'event_row')]\")\n",
    "\n",
    "    for el in items:\n",
    "        cls = (el.get_attribute(\"class\") or \"\").lower()\n",
    "\n",
    "        if \"schedule_date_heading\" in cls:\n",
    "            current_date = (el.text or \"\").strip()\n",
    "            continue\n",
    "\n",
    "        if \"event_row\" in cls:\n",
    "            # Fallback if DOM order doesn't include headings properly:\n",
    "            date_raw = current_date\n",
    "            if not date_raw:\n",
    "                try:\n",
    "                    dh = el.find_element(By.XPATH, \"preceding::*[contains(@class,'schedule_date_heading')][1]\")\n",
    "                    date_raw = (dh.text or \"\").strip()\n",
    "                except Exception:\n",
    "                    date_raw = \"\"\n",
    "\n",
    "            if not date_raw:\n",
    "                continue\n",
    "\n",
    "            # time\n",
    "            try:\n",
    "                time_raw = (el.find_element(By.CLASS_NAME, \"event_time\").text or \"\").strip()\n",
    "            except Exception:\n",
    "                time_raw = \"\"\n",
    "\n",
    "            # game type\n",
    "            try:\n",
    "                game_type = (el.find_element(By.CLASS_NAME, \"game_type\").text or \"\").strip()\n",
    "            except Exception:\n",
    "                game_type = \"\"\n",
    "\n",
    "            if game_type in [\"Scrimmage\", \"Jamboree\"]:\n",
    "                continue\n",
    "\n",
    "            # teams/scores\n",
    "            try:\n",
    "                away_team = (el.find_element(By.CSS_SELECTOR, \".event_team .event_team_name\").text or \"\").strip()\n",
    "                away_score = (el.find_element(By.CSS_SELECTOR, \".event_team .event_team_score\").text or \"\").strip()\n",
    "                home_team = (el.find_element(By.CSS_SELECTOR, \".event_team_home .event_team_name\").text or \"\").strip()\n",
    "                home_score = (el.find_element(By.CSS_SELECTOR, \".event_team_home .event_team_score\").text or \"\").strip()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            away_team = normalize_team_name(away_team)\n",
    "            home_team = normalize_team_name(home_team)\n",
    "\n",
    "            matches.append({\n",
    "                \"Season\": season,\n",
    "                \"LevelID\": level_id,\n",
    "                \"DateRaw\": date_raw,\n",
    "                \"TimeRaw\": time_raw,\n",
    "                \"Home Team\": home_team,\n",
    "                \"Away Team\": away_team,\n",
    "                \"Home Score\": home_score,\n",
    "                \"Away Score\": away_score,\n",
    "                \"Game Type\": game_type,\n",
    "                \"RowIndex\": row_index,  # page order tie-breaker\n",
    "                \"GameID\": generate_game_id(season, date_raw, time_raw, home_team, away_team),\n",
    "            })\n",
    "            row_index += 1\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pipeline\n",
    "# =========================\n",
    "def build_played_order(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"MatchDateTime\"] = [\n",
    "        parse_match_datetime(s, d, t)\n",
    "        for s, d, t in zip(out[\"Season\"], out[\"DateRaw\"], out[\"TimeRaw\"])\n",
    "    ]\n",
    "    out[\"DateISO\"] = pd.to_datetime(out[\"MatchDateTime\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # strict, leakage-resistant ordering:\n",
    "    # chronological, then stable DOM tie-break within season\n",
    "    out[\"MatchDateTimeFill\"] = out[\"MatchDateTime\"].fillna(pd.Timestamp.max)\n",
    "    out = out.sort_values([\"Season\", \"MatchDateTimeFill\", \"RowIndex\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    out[\"MatchOrder\"] = range(1, len(out) + 1)\n",
    "    return out.drop(columns=[\"MatchDateTimeFill\"])\n",
    "\n",
    "\n",
    "def main():\n",
    "    driver = build_driver(HEADLESS)\n",
    "    all_matches = []\n",
    "    accepted_seasons = set()\n",
    "\n",
    "    try:\n",
    "        for season, level_id, url in season_urls():\n",
    "            # accept first level_id that returns real rows for the season\n",
    "            if season in accepted_seasons:\n",
    "                continue\n",
    "\n",
    "            print(f\"Scraping {season} → {url}\")\n",
    "            season_matches = scrape_season(driver, season, level_id, url)\n",
    "\n",
    "            if season_matches:\n",
    "                all_matches.extend(season_matches)\n",
    "                accepted_seasons.add(season)\n",
    "            else:\n",
    "                # try next level_id for that same season\n",
    "                continue\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    if not all_matches:\n",
    "        raise RuntimeError(f\"No matches scraped. Check debug folder: {DEBUG_DIR}\")\n",
    "\n",
    "    df = pd.DataFrame(all_matches).drop_duplicates(subset=[\"GameID\"]).copy()\n",
    "\n",
    "    df = build_played_order(df)\n",
    "\n",
    "    out_csv = OUTPUT_DIR / \"metroleague_results_all_games.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\", out_csv)\n",
    "    print(\"\\nUnique DateRaw sample:\")\n",
    "    for d in pd.Series(df[\"DateRaw\"]).dropna().drop_duplicates().head(20).to_list():\n",
    "        print(\" \", d)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
